2025-07-18 04:09:30,069 - INFO - WandB run initialized: HyperCLOVAX-SEED-Text-Instruct-0.5B_20250718_040928 (ID: 6a0da255)
Fetching 8 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 71089.90it/s]
2025-07-18 04:09:30,395 - INFO - Using scheduler recommended num_fewshot: 0
2025-07-18 04:09:30,395 - INFO - HyperCLOVAX-SEED-Text-Instruct-0.5B_harness_1: Processing task 1/1: humaneval
2025-07-18 04:09:30,395 - INFO - HyperCLOVAX-SEED-Text-Instruct-0.5B_harness_1: Task 'humaneval' detected as zero-shot task
2025-07-18 04:09:30,523 - INFO - HyperCLOVAX-SEED-Text-Instruct-0.5B_harness_1: Evaluating task 'humaneval' with num_fewshot=0
2025-07-18 04:09:30,524 - WARNING - generation_kwargs: {'max_gen_toks': 256} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:00<00:00, 3759.51it/s]
Running generate_until requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [01:55<00:00,  1.42it/s]
Traceback (most recent call last):
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/entrance.py", line 231, in <module>
    main()
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/entrance.py", line 219, in main
    evaluation_lm.main()
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 950, in main
    run_name, error = evaluate_single(model_idx, model_config, [task_priority.task_name], FULL_RUN)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 709, in evaluate_single
    results = evaluate_with_retry("hf", hf_args, task_list, num_fewshot, batch_size, device, limit, gen_kwargs, extra_kwargs, run_name, wandb_run, tracker_context)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 321, in evaluate_with_retry
    result = run_evaluation(model, model_args, [task], num_fewshot, batch_size, device, limit, gen_kwargs, extra_kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 201, in run_evaluation
    return evaluator.simple_evaluate(model=model, model_args=model_args, **eval_params)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/evaluator.py", line 338, in simple_evaluate
    results = evaluate(
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/evaluator.py", line 616, in evaluate
    metrics = task.process_results(
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/api/task.py", line 1711, in process_results
    result_score = self._metric_fn_list[metric](
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/tasks/humaneval/utils.py", line 18, in pass_at_k
    res = compute_.compute(
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/site-packages/evaluate/module.py", line 467, in compute
    output = self._compute(**inputs, **compute_kwargs)
  File "/home/gwlee/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--code_eval/78d307ea938083398db7d9815f03ed661e9c15f60d77880ce007a8a02648f176/code_eval.py", line 179, in _compute
    for future in as_completed(futures):
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/threading.py", line 607, in wait
    signaled = self._cond.wait(timeout)
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/entrance.py", line 231, in <module>
    main()
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/entrance.py", line 219, in main
    evaluation_lm.main()
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 950, in main
    run_name, error = evaluate_single(model_idx, model_config, [task_priority.task_name], FULL_RUN)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 709, in evaluate_single
    results = evaluate_with_retry("hf", hf_args, task_list, num_fewshot, batch_size, device, limit, gen_kwargs, extra_kwargs, run_name, wandb_run, tracker_context)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 321, in evaluate_with_retry
    result = run_evaluation(model, model_args, [task], num_fewshot, batch_size, device, limit, gen_kwargs, extra_kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/core/evaluation_lm.py", line 201, in run_evaluation
    return evaluator.simple_evaluate(model=model, model_args=model_args, **eval_params)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/evaluator.py", line 338, in simple_evaluate
    results = evaluate(
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/evaluator.py", line 616, in evaluate
    metrics = task.process_results(
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/api/task.py", line 1711, in process_results
    result_score = self._metric_fn_list[metric](
  File "/home/gwlee/Benchmark/AIDE_Benchmark/code/lm-evaluation-harness/lm_eval/tasks/humaneval/utils.py", line 18, in pass_at_k
    res = compute_.compute(
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/site-packages/evaluate/module.py", line 467, in compute
    output = self._compute(**inputs, **compute_kwargs)
  File "/home/gwlee/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--code_eval/78d307ea938083398db7d9815f03ed661e9c15f60d77880ce007a8a02648f176/code_eval.py", line 179, in _compute
    for future in as_completed(futures):
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/threading.py", line 607, in wait
    signaled = self._cond.wait(timeout)
  File "/home/gwlee/miniconda3/envs/science/lib/python3.10/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt
