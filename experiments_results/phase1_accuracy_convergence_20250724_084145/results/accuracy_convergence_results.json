{
  "20": {
    "limit": 20,
    "execution_time": 12816.874484062195,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.3288888888888889,
        "kmmlu_hard": 0.26,
        "haerae": 0.48,
        "kobest": 0.6,
        "csatqa": 0.2702702702702703,
        "kormedmcqa": 0.0875,
        "mmlu": 0.6622807017543859,
        "arc_challenge": 0.4,
        "arc_easy": 0.85,
        "hellaswag": 0.45
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.37333333333333335,
        "kmmlu_hard": 0.25666666666666665,
        "haerae": 0.81,
        "kobest": 0.72,
        "csatqa": 0.4774774774774775,
        "kormedmcqa": 0.5875,
        "mmlu": 0.7385964912280701,
        "arc_challenge": 0.65,
        "arc_easy": 0.8,
        "hellaswag": 0.6
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.30444444444444446,
        "kmmlu_hard": 0.25,
        "haerae": 0.44,
        "kobest": 0.53,
        "csatqa": 0.22522522522522523,
        "kormedmcqa": 0.4375,
        "mmlu": 0.6087719298245614,
        "arc_challenge": 0.45,
        "arc_easy": 0.85,
        "hellaswag": 0.45
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.43333333333333335,
        "kmmlu_hard": 0.27,
        "haerae": 0.67,
        "kobest": 0.63,
        "csatqa": 0.46846846846846846,
        "kormedmcqa": 0.25,
        "mmlu": 0.7614035087719299,
        "arc_challenge": 0.45,
        "arc_easy": 0.75,
        "hellaswag": 0.4
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.43444444444444447,
        "kmmlu_hard": 0.2822222222222222,
        "haerae": 0.68,
        "kobest": 0.75,
        "csatqa": 0.40540540540540543,
        "kormedmcqa": 0.575,
        "mmlu": 0.6640350877192982,
        "arc_challenge": 0.55,
        "arc_easy": 0.8,
        "hellaswag": 0.45
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.4211111111111111,
        "kmmlu_hard": 0.24,
        "haerae": 0.87,
        "kobest": 0.76,
        "csatqa": 0.4864864864864865,
        "kormedmcqa": 0.6125,
        "mmlu": 0.7070175438596491,
        "arc_challenge": 0.6,
        "arc_easy": 0.85,
        "hellaswag": 0.5
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.23444444444444446,
        "kmmlu_hard": 0.2,
        "haerae": 0.39,
        "kobest": 0.54,
        "csatqa": 0.25225225225225223,
        "kormedmcqa": 0.4375,
        "mmlu": 0.6482456140350877,
        "arc_challenge": 0.6,
        "arc_easy": 0.8,
        "hellaswag": 0.6
      }
    },
    "statistics": {
      "avg_accuracy": 0.5196046478809637,
      "std_accuracy": 0.19335556750670543,
      "min_accuracy": 0.0875,
      "max_accuracy": 0.87,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-24T12:15:22.528641"
  },
  "40": {
    "limit": 40,
    "execution_time": 19516.505543231964,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.32222222222222224,
        "kmmlu_hard": 0.24575311438278596,
        "haerae": 0.465,
        "kobest": 0.545,
        "csatqa": 0.2810810810810811,
        "kormedmcqa": 0.0625,
        "mmlu": 0.6486842105263158,
        "arc_challenge": 0.475,
        "arc_easy": 0.85,
        "hellaswag": 0.5
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.38333333333333336,
        "kmmlu_hard": 0.24971687429218573,
        "haerae": 0.745,
        "kobest": 0.69,
        "csatqa": 0.5081081081081081,
        "kormedmcqa": 0.55,
        "mmlu": 0.7276315789473684,
        "arc_challenge": 0.675,
        "arc_easy": 0.75,
        "hellaswag": 0.6
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.31055555555555553,
        "kmmlu_hard": 0.23782559456398641,
        "haerae": 0.395,
        "kobest": 0.515,
        "csatqa": 0.2702702702702703,
        "kormedmcqa": 0.38125,
        "mmlu": 0.6100877192982456,
        "arc_challenge": 0.55,
        "arc_easy": 0.8,
        "hellaswag": 0.5
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.45,
        "kmmlu_hard": 0.27123442808607023,
        "haerae": 0.685,
        "kobest": 0.655,
        "csatqa": 0.5081081081081081,
        "kormedmcqa": 0.2,
        "mmlu": 0.7513157894736842,
        "arc_challenge": 0.525,
        "arc_easy": 0.8,
        "hellaswag": 0.45
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.46944444444444444,
        "kmmlu_hard": 0.3040770101925255,
        "haerae": 0.665,
        "kobest": 0.755,
        "csatqa": 0.41621621621621624,
        "kormedmcqa": 0.5375,
        "mmlu": 0.6653508771929825,
        "arc_challenge": 0.575,
        "arc_easy": 0.75,
        "hellaswag": 0.5
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.4311111111111111,
        "kmmlu_hard": 0.2559456398640997,
        "haerae": 0.84,
        "kobest": 0.765,
        "csatqa": 0.518918918918919,
        "kormedmcqa": 0.5625,
        "mmlu": 0.7192982456140351,
        "arc_challenge": 0.625,
        "arc_easy": 0.8,
        "hellaswag": 0.525
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.2477777777777778,
        "kmmlu_hard": 0.1970554926387316,
        "haerae": 0.45,
        "kobest": 0.515,
        "csatqa": 0.2810810810810811,
        "kormedmcqa": 0.43125,
        "mmlu": 0.656578947368421,
        "arc_challenge": 0.7,
        "arc_easy": 0.775,
        "hellaswag": 0.6
      }
    },
    "statistics": {
      "avg_accuracy": 0.5239111964381381,
      "std_accuracy": 0.1861581047952702,
      "min_accuracy": 0.0625,
      "max_accuracy": 0.85,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-24T17:40:39.034233"
  },
  "60": {
    "limit": 60,
    "execution_time": 25433.660786390305,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.34185185185185185,
        "kmmlu_hard": 0.24484235110938107,
        "haerae": 0.43666666666666665,
        "kobest": 0.5633333333333334,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.05416666666666667,
        "mmlu": 0.6502923976608187,
        "arc_challenge": 0.48333333333333334,
        "arc_easy": 0.8166666666666667,
        "hellaswag": 0.4666666666666667
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.39037037037037037,
        "kmmlu_hard": 0.24017127286882056,
        "haerae": 0.71,
        "kobest": 0.7033333333333334,
        "csatqa": 0.5080213903743316,
        "kormedmcqa": 0.5708333333333333,
        "mmlu": 0.72953216374269,
        "arc_challenge": 0.65,
        "arc_easy": 0.7833333333333333,
        "hellaswag": 0.5833333333333334
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.32,
        "kmmlu_hard": 0.2417282989490074,
        "haerae": 0.39,
        "kobest": 0.5366666666666666,
        "csatqa": 0.26737967914438504,
        "kormedmcqa": 0.35,
        "mmlu": 0.6099415204678362,
        "arc_challenge": 0.5166666666666667,
        "arc_easy": 0.7833333333333333,
        "hellaswag": 0.48333333333333334
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.46296296296296297,
        "kmmlu_hard": 0.26741922927209033,
        "haerae": 0.67,
        "kobest": 0.6633333333333333,
        "csatqa": 0.5133689839572193,
        "kormedmcqa": 0.20833333333333334,
        "mmlu": 0.7570175438596491,
        "arc_challenge": 0.55,
        "arc_easy": 0.8166666666666667,
        "hellaswag": 0.45
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.47888888888888886,
        "kmmlu_hard": 0.30167380303620084,
        "haerae": 0.6333333333333333,
        "kobest": 0.76,
        "csatqa": 0.42245989304812837,
        "kormedmcqa": 0.5541666666666667,
        "mmlu": 0.6690058479532164,
        "arc_challenge": 0.6,
        "arc_easy": 0.7666666666666667,
        "hellaswag": 0.48333333333333334
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.4425925925925926,
        "kmmlu_hard": 0.25885558583106266,
        "haerae": 0.83,
        "kobest": 0.77,
        "csatqa": 0.5187165775401069,
        "kormedmcqa": 0.5708333333333333,
        "mmlu": 0.7204678362573099,
        "arc_challenge": 0.6166666666666667,
        "arc_easy": 0.8,
        "hellaswag": 0.5333333333333333
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.25222222222222224,
        "kmmlu_hard": 0.191124951342935,
        "haerae": 0.4033333333333333,
        "kobest": 0.5466666666666666,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.44166666666666665,
        "mmlu": 0.6570175438596492,
        "arc_challenge": 0.7333333333333333,
        "arc_easy": 0.8,
        "hellaswag": 0.5833333333333334
      }
    },
    "statistics": {
      "avg_accuracy": 0.5241633906516642,
      "std_accuracy": 0.1870842495453359,
      "min_accuracy": 0.05416666666666667,
      "max_accuracy": 0.83,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-25T00:44:32.695067"
  },
  "80": {
    "limit": 80,
    "execution_time": 30165.012169361115,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.34833333333333333,
        "kmmlu_hard": 0.2445506121230218,
        "haerae": 0.4225,
        "kobest": 0.555,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.0625,
        "mmlu": 0.6427631578947368,
        "arc_challenge": 0.475,
        "arc_easy": 0.825,
        "hellaswag": 0.5125
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.4066666666666667,
        "kmmlu_hard": 0.23798148701104807,
        "haerae": 0.7,
        "kobest": 0.6975,
        "csatqa": 0.5080213903743316,
        "kormedmcqa": 0.596875,
        "mmlu": 0.7254385964912281,
        "arc_challenge": 0.6,
        "arc_easy": 0.8125,
        "hellaswag": 0.6125
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.3194444444444444,
        "kmmlu_hard": 0.24664078829501343,
        "haerae": 0.3875,
        "kobest": 0.535,
        "csatqa": 0.26737967914438504,
        "kormedmcqa": 0.35,
        "mmlu": 0.6050438596491228,
        "arc_challenge": 0.4875,
        "arc_easy": 0.7875,
        "hellaswag": 0.5375
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.47583333333333333,
        "kmmlu_hard": 0.2675425500149298,
        "haerae": 0.635,
        "kobest": 0.655,
        "csatqa": 0.5133689839572193,
        "kormedmcqa": 0.1875,
        "mmlu": 0.7557017543859649,
        "arc_challenge": 0.525,
        "arc_easy": 0.8375,
        "hellaswag": 0.5125
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.4905555555555556,
        "kmmlu_hard": 0.3042699313227829,
        "haerae": 0.615,
        "kobest": 0.745,
        "csatqa": 0.42245989304812837,
        "kormedmcqa": 0.58125,
        "mmlu": 0.6653508771929825,
        "arc_challenge": 0.5875,
        "arc_easy": 0.8,
        "hellaswag": 0.5375
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.45305555555555554,
        "kmmlu_hard": 0.2555986861749776,
        "haerae": 0.8275,
        "kobest": 0.7625,
        "csatqa": 0.5187165775401069,
        "kormedmcqa": 0.571875,
        "mmlu": 0.7212719298245615,
        "arc_challenge": 0.5875,
        "arc_easy": 0.825,
        "hellaswag": 0.5875
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.2588888888888889,
        "kmmlu_hard": 0.1911018214392356,
        "haerae": 0.3875,
        "kobest": 0.55,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.44375,
        "mmlu": 0.6502192982456141,
        "arc_challenge": 0.7,
        "arc_easy": 0.825,
        "hellaswag": 0.625
      }
    },
    "statistics": {
      "avg_accuracy": 0.5275970653099038,
      "std_accuracy": 0.18784949651321603,
      "min_accuracy": 0.0625,
      "max_accuracy": 0.8375,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-25T09:07:17.707284"
  },
  "100": {
    "limit": 100,
    "execution_time": 35596.13167452812,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.3473333333333333,
        "kmmlu_hard": 0.24439571150097467,
        "haerae": 0.426,
        "kobest": 0.546,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.065,
        "mmlu": 0.6429824561403509,
        "arc_challenge": 0.47,
        "arc_easy": 0.82,
        "hellaswag": 0.51
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.41,
        "kmmlu_hard": 0.23464912280701755,
        "haerae": 0.68,
        "kobest": 0.69,
        "csatqa": 0.5080213903743316,
        "kormedmcqa": 0.5875,
        "mmlu": 0.7221052631578947,
        "arc_challenge": 0.63,
        "arc_easy": 0.81,
        "hellaswag": 0.58
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.31933333333333336,
        "kmmlu_hard": 0.2448830409356725,
        "haerae": 0.392,
        "kobest": 0.526,
        "csatqa": 0.26737967914438504,
        "kormedmcqa": 0.3325,
        "mmlu": 0.606140350877193,
        "arc_challenge": 0.49,
        "arc_easy": 0.78,
        "hellaswag": 0.52
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.4751111111111111,
        "kmmlu_hard": 0.26072124756335285,
        "haerae": 0.61,
        "kobest": 0.646,
        "csatqa": 0.5133689839572193,
        "kormedmcqa": 0.1825,
        "mmlu": 0.7536842105263157,
        "arc_challenge": 0.57,
        "arc_easy": 0.83,
        "hellaswag": 0.5
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.4931111111111111,
        "kmmlu_hard": 0.30653021442495126,
        "haerae": 0.61,
        "kobest": 0.734,
        "csatqa": 0.42245989304812837,
        "kormedmcqa": 0.5625,
        "mmlu": 0.6621052631578948,
        "arc_challenge": 0.6,
        "arc_easy": 0.8,
        "hellaswag": 0.54
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.46,
        "kmmlu_hard": 0.25511695906432746,
        "haerae": 0.824,
        "kobest": 0.768,
        "csatqa": 0.5187165775401069,
        "kormedmcqa": 0.565,
        "mmlu": 0.7171929824561404,
        "arc_challenge": 0.61,
        "arc_easy": 0.82,
        "hellaswag": 0.57
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.26222222222222225,
        "kmmlu_hard": 0.19078947368421054,
        "haerae": 0.382,
        "kobest": 0.55,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.4175,
        "mmlu": 0.647719298245614,
        "arc_challenge": 0.71,
        "arc_easy": 0.8,
        "hellaswag": 0.64
      }
    },
    "statistics": {
      "avg_accuracy": 0.5249916878500468,
      "std_accuracy": 0.18663302123204215,
      "min_accuracy": 0.065,
      "max_accuracy": 0.83,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-25T19:00:33.839010"
  },
  "120": {
    "limit": 120,
    "execution_time": 39587.03548192978,
    "accuracy_results": {
      "meta-llama/Llama-3.1-8B": {
        "kmmlu": 0.34830188679245283,
        "kmmlu_hard": 0.24439571150097467,
        "haerae": 0.42,
        "kobest": 0.545,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.06041666666666667,
        "mmlu": 0.6509362155579236,
        "arc_challenge": 0.5166666666666667,
        "arc_easy": 0.8166666666666667,
        "hellaswag": 0.49166666666666664
      },
      "google/gemma-3-12b-it": {
        "kmmlu": 0.4130188679245283,
        "kmmlu_hard": 0.23464912280701755,
        "haerae": 0.67,
        "kobest": 0.7016666666666667,
        "csatqa": 0.5080213903743316,
        "kormedmcqa": 0.56875,
        "mmlu": 0.7293347541482722,
        "arc_challenge": 0.65,
        "arc_easy": 0.8083333333333333,
        "hellaswag": 0.55
      },
      "mistralai/Mistral-7B-v0.3": {
        "kmmlu": 0.3183018867924528,
        "kmmlu_hard": 0.2448830409356725,
        "haerae": 0.39,
        "kobest": 0.5166666666666667,
        "csatqa": 0.26737967914438504,
        "kormedmcqa": 0.32083333333333336,
        "mmlu": 0.6102907596285584,
        "arc_challenge": 0.525,
        "arc_easy": 0.7666666666666667,
        "hellaswag": 0.5
      },
      "Qwen/Qwen3-8B": {
        "kmmlu": 0.48056603773584905,
        "kmmlu_hard": 0.26072124756335285,
        "haerae": 0.5833333333333334,
        "kobest": 0.6466666666666666,
        "csatqa": 0.5133689839572193,
        "kormedmcqa": 0.16875,
        "mmlu": 0.7599330187243112,
        "arc_challenge": 0.575,
        "arc_easy": 0.8166666666666667,
        "hellaswag": 0.48333333333333334
      },
      "dnotitia/Llama-DNA-1.0-8B-Instruct": {
        "kmmlu": 0.4932075471698113,
        "kmmlu_hard": 0.30653021442495126,
        "haerae": 0.585,
        "kobest": 0.7233333333333334,
        "csatqa": 0.42245989304812837,
        "kormedmcqa": 0.55,
        "mmlu": 0.6728573603288172,
        "arc_challenge": 0.625,
        "arc_easy": 0.7916666666666666,
        "hellaswag": 0.5083333333333333
      },
      "LGAI-EXAONE/EXAONE-3.5-32B-Instruct": {
        "kmmlu": 0.46452830188679245,
        "kmmlu_hard": 0.25511695906432746,
        "haerae": 0.8133333333333334,
        "kobest": 0.7616666666666667,
        "csatqa": 0.5187165775401069,
        "kormedmcqa": 0.5583333333333333,
        "mmlu": 0.7212665550312072,
        "arc_challenge": 0.6416666666666667,
        "arc_easy": 0.8333333333333334,
        "hellaswag": 0.5333333333333333
      },
      "saltlux/luxia-21.4b-alignment-v1.2": {
        "kmmlu": 0.26169811320754716,
        "kmmlu_hard": 0.19078947368421054,
        "haerae": 0.38,
        "kobest": 0.545,
        "csatqa": 0.28342245989304815,
        "kormedmcqa": 0.41458333333333336,
        "mmlu": 0.6521540569340843,
        "arc_challenge": 0.725,
        "arc_easy": 0.8166666666666667,
        "hellaswag": 0.6166666666666667
      }
    },
    "statistics": {
      "avg_accuracy": 0.5232181796527626,
      "std_accuracy": 0.18728180565177965,
      "min_accuracy": 0.06041666666666667,
      "max_accuracy": 0.8333333333333334,
      "num_model_task_pairs": 70,
      "num_successful_pairs": 70
    },
    "timestamp": "2025-07-26T06:00:20.874540"
  }
}